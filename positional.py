import torch
import math

d_model = 6
seq_len = 16
# Create a matrix of shape (seq_len, d_model)
pe = torch.zeros(seq_len, d_model)
# Create a vector of shape (seq_len)
position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)
# Create a vector of shape (d_model)
div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)
# Apply sine to even indices
pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))
# Apply cosine to odd indices
pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))
print(f"positional embedding: {pe}")
# positional embedding: tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],
#         [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],
#         [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],
#         [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],
#         [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],
#         [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],
#         [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],
#         [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],
#         [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],
#         [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998],
#         [-0.5440, -0.8391,  0.4477,  0.8942,  0.0215,  0.9998],
#         [-1.0000,  0.0044,  0.4887,  0.8725,  0.0237,  0.9997],
#         [-0.5366,  0.8439,  0.5286,  0.8488,  0.0259,  0.9997],
#         [ 0.4202,  0.9074,  0.5675,  0.8234,  0.0280,  0.9996],
#         [ 0.9906,  0.1367,  0.6050,  0.7962,  0.0302,  0.9995],
#         [ 0.6503, -0.7597,  0.6413,  0.7673,  0.0323,  0.9995]])
torch.set_printoptions(precision=7)
x = torch.tensor([
    [-0.3633453, -0.33893964, 0.5140128, 0.32748395, -0.3561232, -0.5047984],
[-0.48211774, 0.33215308, 0.7394951, 0.078132406, -0.028996263, -0.32161838],
[-0.19790855, -0.034714203, 0.86141354, 0.06291166, -0.40263462, -0.22698326],
[-0.45271215, 0.15769157, 0.7281278, 0.23677413, -0.4258146, -0.05133263],
[-0.77712417, -0.029864147, 0.3315905, 0.48228958, -0.030618956, -0.22736134],
[-0.1465702, -0.55493087, 0.63132393, 0.44898948, -0.053916577, -0.25980854],
[0.09254342, -0.8287471, 0.2785865, -0.4033236, 0.2528741, 0.019709306],
[-0.3082398, 0.20658346, 0.86035013, -0.23405814, -0.25878537, 0.018872492],
[-0.32759163, 0.22439198, 0.68951416, 0.0753833, -0.583626, -0.14352815],
[-0.11340556, 0.13960978, 0.6583482, 0.06108476, -0.68639284, 0.24363792],
[0.5715848, -0.07974044, 0.7571991, 0.1993511, -0.15035093, -0.17673564],
[-0.39767742, -0.13064946, 0.72660667, -0.26339492, -0.41193008, 0.24033913],
[-0.09375604, -0.10740708, 0.7527027, 0.24173702, -0.58054805, -0.13281316],
[-0.26784053, -0.10819349, 0.8627285, 0.10672506, 0.3211737, -0.24023418],
[0.119970955, -0.5766705, 0.5151158, 0.28266576, 0.17140155, -0.5276697],
[-0.62758964, 0.3103241, 0.59859884, 0.18160084, 0.013691519, -0.3440106],
])
print(f"input words embedding: {x}")
# input words embedding: tensor([[-0.3633453, -0.3389396,  0.5140128,  0.3274840, -0.3561232, -0.5047984],
#         [-0.4821177,  0.3321531,  0.7394951,  0.0781324, -0.0289963, -0.3216184],
#         [-0.1979086, -0.0347142,  0.8614135,  0.0629117, -0.4026346, -0.2269833],
#         [-0.4527121,  0.1576916,  0.7281278,  0.2367741, -0.4258146, -0.0513326],
#         [-0.7771242, -0.0298641,  0.3315905,  0.4822896, -0.0306190, -0.2273613],
#         [-0.1465702, -0.5549309,  0.6313239,  0.4489895, -0.0539166, -0.2598085],
#         [ 0.0925434, -0.8287471,  0.2785865, -0.4033236,  0.2528741,  0.0197093],
#         [-0.3082398,  0.2065835,  0.8603501, -0.2340581, -0.2587854,  0.0188725],
#         [-0.3275916,  0.2243920,  0.6895142,  0.0753833, -0.5836260, -0.1435281],
#         [-0.1134056,  0.1396098,  0.6583482,  0.0610848, -0.6863928,  0.2436379],
#         [ 0.5715848, -0.0797404,  0.7571991,  0.1993511, -0.1503509, -0.1767356],
#         [-0.3976774, -0.1306495,  0.7266067, -0.2633949, -0.4119301,  0.2403391],
#         [-0.0937560, -0.1074071,  0.7527027,  0.2417370, -0.5805480, -0.1328132],
#         [-0.2678405, -0.1081935,  0.8627285,  0.1067251,  0.3211737, -0.2402342],
#         [ 0.1199710, -0.5766705,  0.5151158,  0.2826658,  0.1714015, -0.5276697],
#         [-0.6275896,  0.3103241,  0.5985988,  0.1816008,  0.0136915, -0.3440106]])
x = x + pe
print(f"input words embedding with positional info: {x}")
# input words embedding with positional info: tensor([[-0.3633453,  0.6610603,  0.5140128,  1.3274839, -0.3561232,  0.4952016],
#         [ 0.3593532,  0.8724554,  0.7858943,  1.0770555, -0.0268418,  0.6783794],
#         [ 0.7113888, -0.4508610,  0.9541121,  1.0586059, -0.3983258,  0.7730075],
#         [-0.3115922, -0.8323009,  0.8669258,  1.2270948, -0.4193513,  0.9486464],
#         [-1.5339267, -0.6835077,  0.5161892,  1.4651035, -0.0220013,  0.7726016],
#         [-1.1054945, -0.2712687,  0.8613256,  1.4221797, -0.0431446,  0.7401335],
#         [-0.1868721,  0.1314232,  0.5534958,  0.5581466,  0.2658004,  1.0196258],
#         [ 0.3487468,  0.9604857,  1.1795747,  0.7136209, -0.2437049,  1.0187588],
#         [ 0.6617666,  0.0788919,  1.0523665,  1.0072299, -0.5663913,  0.8563233],
#         [ 0.2987129, -0.7715205,  1.0640467,  0.9750917, -0.6670042,  1.2434499],
#         [ 0.0275638, -0.9188120,  1.2048700,  1.0935495, -0.1288082,  0.8230323],
#         [-1.3976676, -0.1262238,  1.2152854,  0.6090689, -0.3882335,  1.2400583],
#         [-0.6303290,  0.7364469,  1.2813368,  1.0905869, -0.5546977,  0.8668526],
#         [ 0.1523265,  0.7992533,  1.4301792,  0.9301324,  0.3491777,  0.7593736],
#         [ 1.1105783, -0.4399333,  1.1201608,  1.0788571,  0.2015591,  0.4718754],
#         [ 0.0226982, -0.4493638,  1.2399349,  0.9488609,  0.0460024,  0.6554673]])